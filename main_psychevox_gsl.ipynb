{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the main jupyter notwbook for the PsycheVox GSL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62.328</td>\n",
       "      <td>63.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>68.978</td>\n",
       "      <td>70.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>75.028</td>\n",
       "      <td>78.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>83.808</td>\n",
       "      <td>84.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>88.458</td>\n",
       "      <td>89.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>598.238</td>\n",
       "      <td>599.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>600.608</td>\n",
       "      <td>601.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>602.738</td>\n",
       "      <td>603.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>618.308</td>\n",
       "      <td>619.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>620.538</td>\n",
       "      <td>621.268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_time  stop_time\n",
       "6        62.328     63.178\n",
       "9        68.978     70.288\n",
       "12       75.028     78.128\n",
       "14       83.808     84.588\n",
       "16       88.458     89.968\n",
       "..          ...        ...\n",
       "165     598.238    599.708\n",
       "166     600.608    601.378\n",
       "168     602.738    603.478\n",
       "171     618.308    619.078\n",
       "173     620.538    621.268\n",
       "\n",
       "[87 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store transcript file in a DataFrame\n",
    "transcripts = pd.read_table(\"TRANSCRIPT/300_TRANSCRIPT.csv\")\n",
    "#We keep the rows corresponding to the participant (patient)\n",
    "patient_times = transcripts.loc[transcripts['speaker'] == 'Participant']\n",
    "#We drop the columns for speaker and value (words spoken) and we keep just start and end time of participant speech\n",
    "patient_times.drop(['speaker','value'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split audio file: This is just for the first row (first dialog) of the participant\n",
    "#Select the audio file from the patient to be sliced\n",
    "input_audio_file = \"AUDIO/300_AUDIO.wav\"\n",
    "#Set the folder and name of file to export the sliced audio\n",
    "output_slice_folder = \"audio_sliced/300-1.wav\" #300 stands for the participant number and -1 to the first slice\n",
    "#Store the data from the audio file\n",
    "sound1 = AudioSegment.from_file(input_audio_file, format=\"wav\")\n",
    "#Set the start and end time for the interval to be cut\n",
    "start, end = patient_times.iloc[0]['start_time'], patient_times.iloc[0]['stop_time']\n",
    "#export the file from the start to the end (*1000 is to transform from seconds to milliseconds) to the selected folder for output (outout_slice_folder)\n",
    "\n",
    "# #EN el caso que se junten los pedazos del audio en uno solo:\n",
    "# sound2 = AudioSegment.from_file(input_audio_file, format=\"wav\")\n",
    "# start2, end2 = patient_times.iloc[1]['start_time'], patient_times.iloc[1]['stop_time']\n",
    "# audios_juntos = sound1.append(sound2, crossfade=0)\n",
    "# #Aca termina la parte de juntar los audios\n",
    "\n",
    "file_handle = sound1[start*1000:end*1000].export(output_slice_folder, format=\"wav\")\n",
    "\n",
    "#QUEDA VER SI JUNTAMOS LOS AUDIOS CORTADOS DE CADA UNO O SI LOS ANALIZAMOS POR SEPARADO CADA FRAGMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract features from audio files using OpenSMILE\n",
    "\n",
    "import opensmile\n",
    "#Set audio file path to analyze\n",
    "audio_file_extract = 'audio_sliced/300-1.wav'\n",
    "#Set output path for extracted features file\n",
    "extracted_features_output = 'extracted_features/feat1.csv'\n",
    "\n",
    "#Set opensmile parameters\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")\n",
    "\n",
    "#Extract audio features from selected file\n",
    "extracted_features = smile.process_file(audio_file_extract)\n",
    "\n",
    "#Export extracted features dataframe to a csv\n",
    "extracted_features.to_csv(extracted_features_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhkshdfds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "728ea3f257432b038ae8c77d2c05332ace39b4f16c7149664469bec4529df97b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
